
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Predictive Modeling &#8212; POLS 4728</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=1f4eb643" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/predictive';</script>
    <link rel="icon" href="../_static/favicon_square_very_bold.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Regression" href="linreg.html" />
    <link rel="prev" title="Using LLMs" href="evals_and_prompting.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/ML_logo_simple.svg" class="logo__image only-light" alt="POLS 4728 - Home"/>
    <script>document.write(`<img src="../_static/ML_logo_simple.svg" class="logo__image only-dark" alt="POLS 4728 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Information</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../schedule.html">Schedule</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="learning.html">Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="evals_and_prompting.html">Using LLMs</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Predictive Modeling</a></li>




<li class="toctree-l1"><a class="reference internal" href="linreg.html">Linear Regression</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../reference/prelims_and_digressions.html">Preliminaries and Digressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/hugging_face.html">Hugging Face I</a></li>




<li class="toctree-l1"><a class="reference internal" href="../reference/solutions.html">search_exclude: true</a></li>

<li class="toctree-l1"><a class="reference internal" href="../reference/bibliography.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/alexanderthclark/pols4728/main?urlpath=lab/tree/book/chapters/predictive.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/alexanderthclark/pols4728/blob/main/book/chapters/predictive.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/alexanderthclark/pols4728" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/alexanderthclark/pols4728/issues/new?title=Issue%20on%20page%20%2Fchapters/predictive.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/predictive.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Predictive Modeling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Predictive Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-hat-y">Predicting <span class="math notranslate nohighlight">\(\hat{y}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tufte-s-midterm-elections-model">Tufte’s Midterm Elections Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1">Model 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2">Model 2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biasvariance-decomposition-supervised-prediction">Bias–Variance Decomposition (supervised prediction)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-decomposition-at-a-fixed-x">Pointwise decomposition (at a fixed <span class="math notranslate nohighlight">\(x\)</span>)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#averaged-over-the-test-distribution-of-x">Averaged over the test distribution of <span class="math notranslate nohighlight">\(X\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biasvariance-decomposition">Bias–Variance Decomposition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculation">Calculation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Interpretation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#james-stein">James-Stein</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="predictive-modeling">
<span id="predictive"></span><h1>Predictive Modeling<a class="headerlink" href="#predictive-modeling" title="Link to this heading">#</a></h1>
<p>In this section we discuss the general predictive modeling process, model tuning, the problem of overfitting, and cross-validation. This draws on <span id="id1">[<a class="reference internal" href="../reference/bibliography.html#id3" title="Max Kuhn, Kjell Johnson, and others. Applied predictive modeling. Volume 26. Springer, 2013. URL: https://link-springer-com.ezproxy.cul.columbia.edu/book/10.1007/978-1-4614-6849-3.">Kuhn <em>et al.</em>, 2013</a>]</span> Chapters 2 and 4. ISL is also a good reference for the main ideas. However, I don’t recommend either book for its treatment of cross-validation. Instead, consider <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">the scikit-learn documentation</a> and</p>
<p>Both ISL (Ch 2) and APM (Ch 2 and 4) are suitable references.</p>
<p>We also introduce AUC as a performance measure for binary classifiers. After the corresponding lectures, you should be prepared to read <span id="id2">[<a class="reference internal" href="../reference/bibliography.html#id20" title="Michael D Ward, Brian D Greenhill, and Kristin M Bakke. The perils of policy by p-value: predicting civil conflicts. Journal of Peace Research, 47(4):363–375, 2010.">Ward <em>et al.</em>, 2010</a>]</span> and <span id="id3">[<a class="reference internal" href="../reference/bibliography.html#id16" title="Marcel Neunhoeffer and Sebastian Sternberg. How cross-validation can go wrong and what to do about it. Political Analysis, 27(1):101–106, 2019.">Neunhoeffer and Sternberg, 2019</a>]</span>.</p>
<p>This section introduces key concepts in the predictive modeling process, drawing on <span id="id4">[<a class="reference internal" href="../reference/bibliography.html#id3" title="Max Kuhn, Kjell Johnson, and others. Applied predictive modeling. Volume 26. Springer, 2013. URL: https://link-springer-com.ezproxy.cul.columbia.edu/book/10.1007/978-1-4614-6849-3.">Kuhn <em>et al.</em>, 2013</a>]</span> Chapters 2-4 and <span id="id5">[<a class="reference internal" href="../reference/bibliography.html#id14" title="Sudhir Varma and Richard Simon. Bias in error estimation when using cross-validation for model selection. BMC bioinformatics, 7(1):91, 2006.">Varma and Simon, 2006</a>]</span>. After the corresponding lecture, you should be prepared to read <span id="id6">[<a class="reference internal" href="../reference/bibliography.html#id20" title="Michael D Ward, Brian D Greenhill, and Kristin M Bakke. The perils of policy by p-value: predicting civil conflicts. Journal of Peace Research, 47(4):363–375, 2010.">Ward <em>et al.</em>, 2010</a>]</span> and <span id="id7">[<a class="reference internal" href="../reference/bibliography.html#id16" title="Marcel Neunhoeffer and Sebastian Sternberg. How cross-validation can go wrong and what to do about it. Political Analysis, 27(1):101–106, 2019.">Neunhoeffer and Sternberg, 2019</a>]</span>.</p>
<p>Read ISL chapters 2 and 5.</p>
<section id="predicting-hat-y">
<h2>Predicting <span class="math notranslate nohighlight">\(\hat{y}\)</span><a class="headerlink" href="#predicting-hat-y" title="Link to this heading">#</a></h2>
<p>Most of the machine learning models we will cover focus on prediction. In this world, we are not interested in marginal effects like the increase in wages attributable to schooling, standard errors, or even interpretability. Instead, we focus on some measure of predictive accuracy. A black box model is fine and a simpler model might only be preferred with parsimony as a tiebreaker.</p>
</section>
<section id="tufte-s-midterm-elections-model">
<h2>Tufte’s Midterm Elections Model<a class="headerlink" href="#tufte-s-midterm-elections-model" title="Link to this heading">#</a></h2>
<p>Let’s focus on regression problems where <span class="math notranslate nohighlight">\(y\)</span> is a continuous scalar value. For concreteness, let’s say we are predicting midterm election vote share like in <span id="id8">[<a class="reference internal" href="../reference/bibliography.html#id18" title="Edward R Tufte. Determinants of the outcomes of midterm congressional elections. American Political Science Review, 69(3):812–826, 1975.">Tufte, 1975</a>]</span>. The <span class="math notranslate nohighlight">\(y\)</span> variable is the percentage-point difference between the president’s party’s midterm two-party House vote share and that party’s normal vote (average over the previous eight House elections).</p>
<p><span id="id9">[<a class="reference internal" href="../reference/bibliography.html#id18" title="Edward R Tufte. Determinants of the outcomes of midterm congressional elections. American Political Science Review, 69(3):812–826, 1975.">Tufte, 1975</a>]</span> finds an equation</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Vote Loss}} = -11.08 + 0.035\times \Delta\text{Real Disposable Income} + 0.133\times \text{Presidential Approval}.\]</div>
<p>This line was found based on data from 1938-1970 and the R-squared is 0.912. Recall that R-squared describes how much of the variation in <span class="math notranslate nohighlight">\(y\)</span> is captured by the model. A perfect model gives an R-squared of 1 and a model with only an intercept equal to the average of <span class="math notranslate nohighlight">\(y\)</span> gives an R-squared of 0. Tufte’s 0.912 is impressive. With intro stats training, you might only ask for the adjusted R-squared as a follow up. The adjusted R-squared is also impressive at 0.876.</p>
<p>Adjusted R-squared is a blunt way to prevent being overconfident in your model. After all, we can add new features of random noise, uncorrelated to <span class="math notranslate nohighlight">\(y\)</span> and improve the R-squared. With enough columns, the R-squared will become exactly 1. The adjusted R-squared formula essentially penalizes the R-squared based on the number of predictor variables and the number of observations.</p>
<p>However, the usefulness of this as a predictive model hinges on model performance on elections <em>after</em> 1970. Evaluating the model on a <strong>test set</strong> of previously unseen data is more data driven and it answers the question of interest more exactly. The general principle is that a model should be evaluated with new data that wasn’t used in the model training. Records from 1938-1970 form the <strong>training set</strong> and we can assemble data from 1974-2022 as a <strong>test set</strong>.</p>
<p>Let’s see how two modifications of Tufte’s model holds up with test data. I was unable to replicate Tufte’s Real Disposable Income variable, so I substitute a similar RDI measure using data available from the BEA. In our model 1, we include only the presidential approval variable: <code class="docutils literal notranslate"><span class="pre">vote_loss</span> <span class="pre">~</span> <span class="pre">presidential_approval</span></code>.  Model 2 is spiritually the same as Tufte’s though: <code class="docutils literal notranslate"><span class="pre">vote_loss</span> <span class="pre">~</span> <span class="pre">change_rdi</span> <span class="pre">+</span> <span class="pre">presidential_approval</span></code>.</p>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># hide code and output</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">predictive</span><span class="w"> </span><span class="kn">import</span> <span class="n">calculate_out_of_sample_r2</span>

<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/alexanderthclark/pols4728/refs/heads/main/data/tufte_midterms.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># Tufte analysis</span>
<span class="n">tufte_model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;vote_loss ~ pres_approval + delta_rdi&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">tufte_model_sub</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;vote_loss ~ pres_approval + dpi_pc_pct_yoy&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">out_of_sample_average</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;</span> <span class="mi">1970</span><span class="p">]</span><span class="o">.</span><span class="n">vote_loss</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">13</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">tufte_approval_only</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;vote_loss ~ pres_approval&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;single_lin_reg_residuals&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">vote_loss</span> <span class="o">-</span> <span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">pres_approval</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;multi_reg_residuals&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">vote_loss</span> <span class="o">-</span> <span class="n">tufte_model_sub</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;pres_approval&#39;</span><span class="p">,</span> <span class="s1">&#39;dpi_pc_pct_yoy&#39;</span><span class="p">]])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;multi_reg_predictions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tufte_model_sub</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;pres_approval&#39;</span><span class="p">,</span> <span class="s1">&#39;dpi_pc_pct_yoy&#39;</span><span class="p">]])</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">]</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;</span> <span class="mi">1970</span><span class="p">]</span>

<span class="c1">#calculate_out_of_sample_r2(tufte_approval_only, df2)</span>
<span class="c1">#calculate_out_of_sample_r2(tufte_model_sub, df2)</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="model-1">
<h3>Model 1<a class="headerlink" href="#model-1" title="Link to this heading">#</a></h3>
<p>Model 1 has an R-squared of 0.253 in the training set. From the scatter plot below, we can see that the regression line is sane but imperfect when compared to the test data. The R-squared is just 0.015 on the test set. That means using the regression model is not much better than just guessing the average from the post-1970 data.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hide code, show output</span>
<span class="n">eq</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{y}</span><span class="s2"> = &quot;</span> <span class="o">+</span> <span class="sa">fr</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">+.1f</span><span class="si">}{</span><span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">+.1f</span><span class="si">}</span><span class="s2">x$&quot;</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">vote_loss</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">pres_approval</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">vote_loss</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">pres_approval</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Standardized Vote Loss&quot;</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Presidential Approval Rating&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">eq</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">x0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">pres_approval</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">df1</span><span class="o">.</span><span class="n">pres_approval</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">x0</span><span class="p">])</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">x1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">],</span> <span class="p">[</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>

<span class="c1"># residuals</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residual by year&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">single_lin_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">single_lin_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Residual&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Year&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Model 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/4e7e6661aae4bf49a79a9c3c632e00eefeb144658d11cdbfb0f5f48c3e3891e9.png" src="../_images/4e7e6661aae4bf49a79a9c3c632e00eefeb144658d11cdbfb0f5f48c3e3891e9.png" />
</div>
</div>
</section>
<section id="model-2">
<h3>Model 2<a class="headerlink" href="#model-2" title="Link to this heading">#</a></h3>
<p>Model 1 has an R-squared of 0.876 in the training set. From the residual scatter below, we see that the residuals become more volatile over time. Indeed, the R-squared is -0.233 (yes, negative) for the test data. That means using the regression model is worse than just guessing the average from the post-1970 data.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># hide code, show output </span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">vote_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">vote_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Observed&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Predicted vs Actual&quot;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">multi_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Residual&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Year&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residual by year&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Model 2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/6a7ecba1ee02d9bde09bbf3d83a6ae5c9350a18606259c464e21375ed700194d.png" src="../_images/6a7ecba1ee02d9bde09bbf3d83a6ae5c9350a18606259c464e21375ed700194d.png" />
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="bias-variance-tradeoff">
<h1>Bias-Variance Tradeoff<a class="headerlink" href="#bias-variance-tradeoff" title="Link to this heading">#</a></h1>
<section id="biasvariance-decomposition-supervised-prediction">
<h2>Bias–Variance Decomposition (supervised prediction)<a class="headerlink" href="#biasvariance-decomposition-supervised-prediction" title="Link to this heading">#</a></h2>
<p><strong>Data-generating process</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y = f(X) + \varepsilon\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon \mid X] = 0\)</span>, <span class="math notranslate nohighlight">\(\ \mathbb{Var}(\varepsilon \mid X) = \sigma^2(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat f\)</span> is a fitted model (random through the training sample/algorithm)</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="pointwise-decomposition-at-a-fixed-x">
<h2>Pointwise decomposition (at a fixed <span class="math notranslate nohighlight">\(x\)</span>)<a class="headerlink" href="#pointwise-decomposition-at-a-fixed-x" title="Link to this heading">#</a></h2>
<p>Define the test MSE at <span class="math notranslate nohighlight">\(x\)</span>:
$<span class="math notranslate nohighlight">\(
\mathrm{MSE}(x) \;=\; \mathbb{E}\!\left[(Y - \hat f(x))^2 \mid X=x\right].
\)</span>$</p>
<p>Substitute <span class="math notranslate nohighlight">\(Y=f(x)+\varepsilon\)</span>:
$$
\begin{aligned}
\mathrm{MSE}(x)
&amp;= \mathbb{E}!\left[(f(x)+\varepsilon-\hat f(x))^2 \mid X=x\right] \
&amp;= \mathbb{E}!\left[(f(x)-\hat f(x))^2 \mid X=x\right]</p>
<ul class="simple">
<li><p>\mathbb{E}[\varepsilon^2 \mid X=x]</p></li>
<li><p>2,\mathbb{E}!\left[\varepsilon{f(x)-\hat f(x)}\mid X=x\right].
\end{aligned}
$$</p></li>
</ul>
<p>Because <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon\mid X=x]=0\)</span> and the test noise <span class="math notranslate nohighlight">\(\varepsilon\)</span> is independent of the training randomness in <span class="math notranslate nohighlight">\(\hat f\)</span>, the cross term is <span class="math notranslate nohighlight">\(0\)</span>. Hence
$<span class="math notranslate nohighlight">\(
\mathrm{MSE}(x) \;=\; \underbrace{\mathbb{E}\!\left[(f(x)-\hat f(x))^2\right]}_{\text{estimation error}}
\;+\; \underbrace{\sigma^2(x)}_{\text{irreducible noise}}.
\)</span>$</p>
<p>Now split the estimation error into bias<span class="math notranslate nohighlight">\(^2\)</span> and variance. Let <span class="math notranslate nohighlight">\(\mu(x):=\mathbb{E}[\hat f(x)]\)</span> (expectation over training randomness):
$$
\begin{aligned}
\mathbb{E}!\left[(f(x)-\hat f(x))^2\right]
&amp;= \mathbb{E}!\left[(\hat f(x)-\mu(x) + \mu(x)-f(x))^2\right] \
&amp;= \mathbb{E}!\left[(\hat f(x)-\mu(x))^2\right]</p>
<ul class="simple">
<li><p>(\mu(x)-f(x))^2</p></li>
<li><p>2,\mathbb{E}!\left[(\hat f(x)-\mu(x))(\mu(x)-f(x))\right] \
&amp;= \underbrace{\mathbb{Var}(\hat f(x))}_{\text{variance}}</p></li>
<li><p>\underbrace{(\mathbb{E}[\hat f(x)]-f(x))^2}_{\text{bias}^2},
\end{aligned}
$$
since the cross term is zero (the second factor is constant).</p></li>
</ul>
<p><strong>Therefore</strong>
$$
\boxed{\ \mathrm{MSE}(x)
= \underbrace{(\mathbb{E}[\hat f(x)]-f(x))^2}_{\text{bias}^2(x)}</p>
<ul class="simple">
<li><p>\underbrace{\mathbb{Var}(\hat f(x))}_{\text{variance}(x)}</p></li>
<li><p>\underbrace{\sigma^2(x)}_{\text{irreducible noise}}\ }.
$$</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="averaged-over-the-test-distribution-of-x">
<h2>Averaged over the test distribution of <span class="math notranslate nohighlight">\(X\)</span><a class="headerlink" href="#averaged-over-the-test-distribution-of-x" title="Link to this heading">#</a></h2>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_X[\mathrm{MSE}(x)]
= \mathbb{E}_X[\text{bias}^2(x)]
+ \mathbb{E}_X[\text{variance}(x)]
+ \mathbb{E}_X[\sigma^2(x)].
\]</div>
<p>If noise is homoskedastic, <span class="math notranslate nohighlight">\(\sigma^2(x)\equiv\sigma^2\)</span>, the last term is just <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
</section>
<hr class="docutils" />
<section id="interpretation">
<h2>Interpretation<a class="headerlink" href="#interpretation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Bias</strong>: systematic error from model misspecification/rigidity.</p></li>
<li><p><strong>Variance</strong>: sensitivity to the particular training sample.</p></li>
<li><p><strong>Noise</strong>: randomness you cannot remove.</p></li>
</ul>
<p>Making the model more flexible typically <strong>reduces bias</strong> but <strong>increases variance</strong>; restricting it does the opposite. Optimal predictive performance sits where bias<span class="math notranslate nohighlight">\(^2\)</span> + variance is minimized (the noise term is fixed).</p>
</section>
<section id="biasvariance-decomposition">
<h2>Bias–Variance Decomposition<a class="headerlink" href="#biasvariance-decomposition" title="Link to this heading">#</a></h2>
<p><strong>Data-generating process</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y = f(X) + \varepsilon\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon \mid X] = 0\)</span>, <span class="math notranslate nohighlight">\(\ \mathbb{Var}(\varepsilon \mid X) = \sigma^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat f\)</span> is a fitted model (its randomness comes from the training sample/algorithm)</p></li>
</ul>
<hr class="docutils" />
<section id="calculation">
<h3>Calculation<a class="headerlink" href="#calculation" title="Link to this heading">#</a></h3>
<p>We decompose the mean squared error:
$<span class="math notranslate nohighlight">\(
\mathrm{MSE}(x)
= \mathbb{E}\!\left[(Y - \hat f(x))^2 \mid X=x\right].
\)</span>$</p>
<p>Substitute <span class="math notranslate nohighlight">\(Y=f(x)+\varepsilon\)</span> and expand:
$$
\begin{aligned}
\mathrm{MSE}(x)
&amp;= \mathbb{E}!\left[(f(x)+\varepsilon-\hat f(x))^2 \mid X=x\right] \
&amp;= \mathbb{E}!\left[(f(x)-\hat f(x))^2\right]</p>
<ul class="simple">
<li><p>\mathbb{E}[\varepsilon^2 \mid X=x]</p></li>
<li><p>2,\mathbb{E}!\left[\varepsilon{f(x)-\hat f(x)}\mid X=x\right].
\end{aligned}
$$</p></li>
</ul>
<p>Because <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon\mid X=x]=0\)</span> and the test noise <span class="math notranslate nohighlight">\(\varepsilon\)</span> is independent of the training randomness in <span class="math notranslate nohighlight">\(\hat f\)</span>, the cross term is <span class="math notranslate nohighlight">\(0\)</span>. Hence
$$
\mathrm{MSE}(x)
= \underbrace{\mathbb{E}!\left[(f(x)-\hat f(x))^2\right]}_{\text{estimation error}}</p>
<ul class="simple">
<li><p>\underbrace{\sigma^2}_{\text{irreducible noise}}.
$$</p></li>
</ul>
<p>Now split the estimation error into bias<span class="math notranslate nohighlight">\(^2\)</span> and variance. Add and subtract <span class="math notranslate nohighlight">\(\mathbb{E}[\hat f(x)]\)</span> inside the square:
$$
\begin{aligned}
\mathbb{E}!\left[(f(x)-\hat f(x))^2\right]
&amp;= \mathbb{E}!\left[(\hat f(x)-\mathbb{E}[\hat f(x)] + \mathbb{E}[\hat f(x)]-f(x))^2\right] \
&amp;= \mathbb{E}!\left[(\hat f(x)-\mathbb{E}[\hat f(x)])^2\right]</p>
<ul class="simple">
<li><p>\big(\mathbb{E}[\hat f(x)]-f(x)\big)^2 \quad + 2\big(\mathbb{E}[\hat f(x)]-f(x)\big),\mathbb{E}!\left[\hat f(x)-\mathbb{E}[\hat f(x)]\right]. \
\end{aligned}
$$</p></li>
</ul>
<p>The first term is the variance of <span class="math notranslate nohighlight">\(\hat{f}\)</span> by definition, the second is the squared bias of <span class="math notranslate nohighlight">\(\hat{f}\)</span>, and the third term drops out because <span class="math notranslate nohighlight">\(\mathbb{E}[\hat f(x)-\mathbb{E}[\hat f(x)]]=0\)</span>,</p>
<p>\begin{aligned}
\mathbb{E}!\left[(f(x)-\hat f(x))^2\right] &amp;= \underbrace{\mathbb{Var}(\hat f(x))}_{\text{variance}}</p>
<ul class="simple">
<li><p>\underbrace{\big(\mathbb{E}[\hat f(x)]-f(x)\big)^2}_{\text{bias}^2}.
\end{aligned}</p></li>
</ul>
<p>Therefore,
$$
\boxed{\ \mathrm{MSE}(x)
= \underbrace{\big(\mathbb{E}[\hat f(x)]-f(x)\big)^2}_{\text{bias}^2(x)}</p>
<ul class="simple">
<li><p>\underbrace{\mathbb{Var}(\hat f(x))}_{\text{variance}(x)}</p></li>
<li><p>\underbrace{\sigma^2}_{\text{irreducible noise}}\ }.
$$</p></li>
</ul>
</section>
</section>
<section id="id10">
<h2>Interpretation<a class="headerlink" href="#id10" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Bias</strong>: systematic error from the learning procedure (e.g., underfitting).</p></li>
<li><p><strong>Variance</strong>: sensitivity of <span class="math notranslate nohighlight">\(\hat f(x)\)</span> to the particular training sample (e.g., overfitting).</p></li>
<li><p><strong>Noise</strong>: randomness in <span class="math notranslate nohighlight">\(Y\)</span> that modeling cannot remove.</p></li>
</ul>
<p>Increasing model flexibility typically <strong>reduces bias</strong> but <strong>increases variance</strong>; restricting it does the opposite. Optimal predictive performance sits where bias<span class="math notranslate nohighlight">\(^2\)</span> + variance is minimized (the noise term is fixed).</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="james-stein">
<h1>James-Stein<a class="headerlink" href="#james-stein" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>   <span class="c1"># any fixed 3-vector</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>   <span class="c1"># any fixed 3-vector</span>

<span class="n">R</span> <span class="o">=</span> <span class="mi">1_000_000</span>                             <span class="c1"># Monte Carlo replications</span>

<span class="c1"># One-shot normal-means: Y ~ N(theta, I_p)</span>
<span class="n">Y</span>  <span class="o">=</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>
<span class="n">r2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># James–Stein shrinkage factors (σ² = 1)</span>
<span class="n">a</span>  <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">r2</span>                   <span class="c1"># untruncated JS</span>
<span class="n">ap</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>                 <span class="c1"># positive-part JS</span>

<span class="c1"># Squared-error losses</span>
<span class="n">mle_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">Y</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mle_se2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Y</span><span class="o">-</span><span class="n">theta</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">js_se2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">a</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="o">*</span> <span class="n">Y</span><span class="p">)</span> <span class="o">-</span><span class="n">theta</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="n">js_se</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">a</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="o">*</span> <span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">jsp_se</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(((</span><span class="n">ap</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Normal-means (p=3, σ²=1)&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE risk ≈ </span><span class="si">{</span><span class="n">mle_se</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MLE risk ≈ </span><span class="si">{</span><span class="n">mle_se2</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;JS  risk ≈ </span><span class="si">{</span><span class="n">js_se2</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;JS  risk ≈ </span><span class="si">{</span><span class="n">js_se</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;JS+ risk ≈ </span><span class="si">{</span><span class="n">jsp_se</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Normal-means (p=3, σ²=1)
MLE risk ≈ 2.998622
MLE risk ≈ 2.998622
JS  risk ≈ 1.982706
JS  risk ≈ 1.982706
JS+ risk ≈ 1.601375
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h1>
<p>We’ve now learned about the predictive modeling process. Model tuning and selection is prone to the problem of over-fitting. Performance measures like R-squared (and adjusted R-squared) are misleading when calculated based on training data. A model’s performance can only truly be measured by using completely new test data that was not involved in the training process. The researcher needs to remain disciplined and use procedures like cross-validation to make model selection more systematic, transparent, and reliable. <a class="reference external" href="https://www.econtalk.org/susan-athey-on-machine-learning-big-data-and-causation/">As noted by Susan Athey</a>, instead of researchers subjectively choosing variables and testing specifications behind the scenes, we can now explicitly use data to determine which variables matter. This technological shift enables what <span id="id11">[<a class="reference internal" href="../reference/bibliography.html#id10" title="Justin Grimmer, Margaret E Roberts, and Brandon M Stewart. Machine learning for social science: an agnostic approach. Annual Review of Political Science, 24(1):395–419, 2021.">Grimmer <em>et al.</em>, 2021</a>]</span> describe as a move away from purely deductive social science toward a more inductive, iterative approach where researchers can discover patterns in data rather than only testing pre-specified hypotheses.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercises">
<h1>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h1>
<div class="exercise admonition" id="tufte">

<p class="admonition-title"><span class="caption-number">Exercise 3 </span></p>
<section id="exercise-content">
<p>Using <code class="docutils literal notranslate"><span class="pre">tufte_midterms.csv</span></code>, compare the test R-squared for different model specifications. What specification performs best?</p>
</section>
</div>
<div class="exercise admonition" id="CV-selection">

<p class="admonition-title"><span class="caption-number">Exercise 4 </span></p>
<section id="exercise-content">
<p>Lisa and Bart are trying to find the best model to predict the quantities of oil underground. Lisa chooses her model by (1) dividing her data into test and training. (2) She compares the performance of several different models and hyperparameter settings using 5-fold cross validation on the the training set. Each model is trained five times and evaluated on the five different holdout folds. She obtains an estimate of the model performance by averaging over each of the five evaluations. (3) She picks the winning model and tuning parameters based on the CV procedure. (4) She uses the test set at the end to get a final measure of performance for the selected model.</p>
<p>Bart uses a similar procedure but he picks the winner by using the <em>test</em> set. He uses CV to tune different types of models. Then, he compares each tuned model on the test set and picks the model with the best performance.</p>
<p>Who uses cross validation correctly? Lisa? Bart? Both? Neither?</p>
</section>
</div>
<div class="exercise admonition" id="phack">

<p class="admonition-title"><span class="caption-number">Exercise 5 </span></p>
<section id="exercise-content">
<p>In what ways are p-hacking and overfitting similar? In what ways are they different?</p>
</section>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "alexanderthclark/pols4728",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="evals_and_prompting.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Using LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="linreg.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Linear Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Predictive Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-hat-y">Predicting <span class="math notranslate nohighlight">\(\hat{y}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tufte-s-midterm-elections-model">Tufte’s Midterm Elections Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1">Model 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2">Model 2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biasvariance-decomposition-supervised-prediction">Bias–Variance Decomposition (supervised prediction)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-decomposition-at-a-fixed-x">Pointwise decomposition (at a fixed <span class="math notranslate nohighlight">\(x\)</span>)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#averaged-over-the-test-distribution-of-x">Averaged over the test distribution of <span class="math notranslate nohighlight">\(X\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biasvariance-decomposition">Bias–Variance Decomposition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculation">Calculation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Interpretation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#james-stein">James-Stein</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander Clark, Columbia University
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>