
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Predictive Modeling &#8212; POLS 4728</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=7f149d09" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/predictive';</script>
    <link rel="icon" href="../_static/favicon_square_very_bold.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Regression" href="linreg.html" />
    <link rel="prev" title="Using LLMs" href="evals_and_prompting.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/ML_logo_simple.svg" class="logo__image only-light" alt="POLS 4728 - Home"/>
    <script>document.write(`<img src="../_static/ML_logo_simple.svg" class="logo__image only-dark" alt="POLS 4728 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Information</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../schedule.html">Schedule</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="learning.html">Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="evals_and_prompting.html">Using LLMs</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Predictive Modeling</a></li>




<li class="toctree-l1"><a class="reference internal" href="linreg.html">Linear Regression</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../reference/prelims_and_digressions.html">Preliminaries and Digressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/hugging_face.html">Hugging Face I</a></li>




<li class="toctree-l1"><a class="reference internal" href="../reference/solutions.html">Selected Solutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/bibliography.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/alexanderthclark/pols4728/main?urlpath=lab/tree/book/chapters/predictive.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/alexanderthclark/pols4728/blob/main/book/chapters/predictive.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/alexanderthclark/pols4728" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/alexanderthclark/pols4728/issues/new?title=Issue%20on%20page%20%2Fchapters/predictive.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/predictive.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Predictive Modeling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Predictive Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-hat-y">Predicting <span class="math notranslate nohighlight">\(\hat{y}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tufte-s-midterm-elections-model">Tufte’s Midterm Elections Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1">Model 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2">Model 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#could-tufte-have-produced-a-better-model">Could Tufte have produced a better model?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biasvariance-decomposition-supervised-prediction">Bias–Variance Decomposition (supervised prediction)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-and-model-selection">Validation and Model Selection</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-holdout-validation">Simple Holdout Validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation-for-model-performance">Cross Validation (For Model Performance)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nested-cross-validation-for-model-selection-and-model-performance">Nested Cross Validation (For Model Selection and Model Performance)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-nested-cross-validation">Why Nested Cross Validation?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-unbalanced-data-and-other-complications">Time Series, Unbalanced Data, and Other Complications</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="predictive-modeling">
<span id="predictive"></span><h1>Predictive Modeling<a class="headerlink" href="#predictive-modeling" title="Link to this heading">#</a></h1>
<p>In this section we discuss the general predictive modeling process, model tuning, the problem of overfitting, and cross-validation. This draws on <span id="id1">[<a class="reference internal" href="../reference/bibliography.html#id3" title="Max Kuhn, Kjell Johnson, and others. Applied predictive modeling. Volume 26. Springer, 2013. URL: https://link-springer-com.ezproxy.cul.columbia.edu/book/10.1007/978-1-4614-6849-3.">Kuhn <em>et al.</em>, 2013</a>]</span> Chapters 2 and 4. ISL is also a good reference for the main ideas. However, I don’t recommend either book for its treatment of cross-validation. Instead, consider <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">the scikit-learn documentation</a> and</p>
<p>Both ISL (Ch 2) and APM (Ch 2 and 4) are suitable references.</p>
<p>We also introduce AUC as a performance measure for binary classifiers. After the corresponding lectures, you should be prepared to read <span id="id2">[<a class="reference internal" href="../reference/bibliography.html#id21" title="Michael D Ward, Brian D Greenhill, and Kristin M Bakke. The perils of policy by p-value: predicting civil conflicts. Journal of Peace Research, 47(4):363–375, 2010.">Ward <em>et al.</em>, 2010</a>]</span> and <span id="id3">[<a class="reference internal" href="../reference/bibliography.html#id16" title="Marcel Neunhoeffer and Sebastian Sternberg. How cross-validation can go wrong and what to do about it. Political Analysis, 27(1):101–106, 2019.">Neunhoeffer and Sternberg, 2019</a>]</span>.</p>
<p>This section introduces key concepts in the predictive modeling process, drawing on <span id="id4">[<a class="reference internal" href="../reference/bibliography.html#id3" title="Max Kuhn, Kjell Johnson, and others. Applied predictive modeling. Volume 26. Springer, 2013. URL: https://link-springer-com.ezproxy.cul.columbia.edu/book/10.1007/978-1-4614-6849-3.">Kuhn <em>et al.</em>, 2013</a>]</span> Chapters 2-4 and <span id="id5">[<a class="reference internal" href="../reference/bibliography.html#id14" title="Sudhir Varma and Richard Simon. Bias in error estimation when using cross-validation for model selection. BMC bioinformatics, 7(1):91, 2006.">Varma and Simon, 2006</a>]</span>. After the corresponding lecture, you should be prepared to read <span id="id6">[<a class="reference internal" href="../reference/bibliography.html#id21" title="Michael D Ward, Brian D Greenhill, and Kristin M Bakke. The perils of policy by p-value: predicting civil conflicts. Journal of Peace Research, 47(4):363–375, 2010.">Ward <em>et al.</em>, 2010</a>]</span> and <span id="id7">[<a class="reference internal" href="../reference/bibliography.html#id16" title="Marcel Neunhoeffer and Sebastian Sternberg. How cross-validation can go wrong and what to do about it. Political Analysis, 27(1):101–106, 2019.">Neunhoeffer and Sternberg, 2019</a>]</span>.</p>
<p>Read ISL chapters 2 and 5.</p>
<section id="predicting-hat-y">
<h2>Predicting <span class="math notranslate nohighlight">\(\hat{y}\)</span><a class="headerlink" href="#predicting-hat-y" title="Link to this heading">#</a></h2>
<p>Most of the machine learning models we will cover focus on prediction. In this world, we are not interested in marginal effects like the increase in wages attributable to schooling, standard errors, or even interpretability. Instead, we focus on some measure of predictive accuracy. A black box model is fine and a simpler model might only be preferred with parsimony as a tiebreaker.</p>
</section>
<section id="tufte-s-midterm-elections-model">
<h2>Tufte’s Midterm Elections Model<a class="headerlink" href="#tufte-s-midterm-elections-model" title="Link to this heading">#</a></h2>
<p>Let’s focus on regression problems where <span class="math notranslate nohighlight">\(y\)</span> is a continuous scalar value. For concreteness, let’s say we are predicting midterm election vote share like in <span id="id8">[<a class="reference internal" href="../reference/bibliography.html#id19" title="Edward R Tufte. Determinants of the outcomes of midterm congressional elections. American Political Science Review, 69(3):812–826, 1975.">Tufte, 1975</a>]</span>. The <span class="math notranslate nohighlight">\(y\)</span> variable is the percentage-point difference between the president’s party’s midterm two-party House vote share and that party’s normal vote (average over the previous eight House elections).</p>
<p><span id="id9">[<a class="reference internal" href="../reference/bibliography.html#id19" title="Edward R Tufte. Determinants of the outcomes of midterm congressional elections. American Political Science Review, 69(3):812–826, 1975.">Tufte, 1975</a>]</span> finds an equation</p>
<div class="math notranslate nohighlight">
\[\widehat{\text{Vote Loss}} = -11.08 + 0.035\times \Delta\text{Real Disposable Income} + 0.133\times \text{Presidential Approval}.\]</div>
<p>This line was found based on data from 1938-1970 and the R-squared is 0.912. Recall that R-squared describes how much of the variation in <span class="math notranslate nohighlight">\(y\)</span> is captured by the model. A perfect model gives an R-squared of 1 and a model with only an intercept equal to the average of <span class="math notranslate nohighlight">\(y\)</span> gives an R-squared of 0. Tufte’s 0.912 is impressive. With intro stats training, you might only ask for the adjusted R-squared as a follow up. The adjusted R-squared is also impressive at 0.876.</p>
<p>Adjusted R-squared is a blunt way to prevent being overconfident in your model. After all, we can add new features of random noise, uncorrelated to <span class="math notranslate nohighlight">\(y\)</span> and improve the R-squared. With enough columns, the R-squared will become exactly 1. The adjusted R-squared formula essentially penalizes the R-squared based on the number of predictor variables and the number of observations.</p>
<p>However, the usefulness of this as a predictive model hinges on model performance on elections <em>after</em> 1970. Evaluating the model on a <strong>test set</strong> of previously unseen data is more data driven and it answers the question of interest more exactly. The general principle is that a model should be evaluated with new data that wasn’t used in the model training. Records from 1938-1970 form the <strong>training set</strong> and we can assemble data from 1974-2022 as a <strong>test set</strong>.</p>
<p>Let’s see how two modifications of Tufte’s model holds up with test data. I was unable to replicate Tufte’s Real Disposable Income variable, so I substitute a similar RDI measure using data available from the BEA. In our model 1, we include only the presidential approval variable: <code class="docutils literal notranslate"><span class="pre">vote_loss</span> <span class="pre">~</span> <span class="pre">presidential_approval</span></code>.  Model 2 is spiritually the same as Tufte’s though: <code class="docutils literal notranslate"><span class="pre">vote_loss</span> <span class="pre">~</span> <span class="pre">change_rdi</span> <span class="pre">+</span> <span class="pre">presidential_approval</span></code>.</p>
<p>We will see that none of these models performs very well on the test data.</p>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># hide code and output</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.formula.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">smf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">statsmodels.api</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">predictive</span><span class="w"> </span><span class="kn">import</span> <span class="n">calculate_out_of_sample_r2</span>

<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/alexanderthclark/pols4728/refs/heads/main/data/tufte_midterms.csv&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="c1"># Tufte analysis</span>
<span class="n">tufte_model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;vote_loss ~ pres_approval + delta_rdi&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">tufte_model_sub</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;vote_loss ~ pres_approval + dpi_pc_pct_yoy&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">out_of_sample_average</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;</span> <span class="mi">1970</span><span class="p">]</span><span class="o">.</span><span class="n">vote_loss</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">13</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">tufte_approval_only</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;vote_loss ~ pres_approval&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;single_lin_reg_residuals&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">vote_loss</span> <span class="o">-</span> <span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">pres_approval</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;multi_reg_residuals&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">vote_loss</span> <span class="o">-</span> <span class="n">tufte_model_sub</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;pres_approval&#39;</span><span class="p">,</span> <span class="s1">&#39;dpi_pc_pct_yoy&#39;</span><span class="p">]])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;multi_reg_predictions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tufte_model_sub</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;pres_approval&#39;</span><span class="p">,</span> <span class="s1">&#39;dpi_pc_pct_yoy&#39;</span><span class="p">]])</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">in_original</span><span class="o">==</span><span class="kc">True</span><span class="p">]</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;</span> <span class="mi">1970</span><span class="p">]</span>

<span class="c1">#calculate_out_of_sample_r2(tufte_approval_only, df2)</span>
<span class="c1">#calculate_out_of_sample_r2(tufte_model_sub, df2)</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="model-1">
<h3>Model 1<a class="headerlink" href="#model-1" title="Link to this heading">#</a></h3>
<p>Model 1 has an R-squared of 0.253 in the training set. From the scatter plot below, we can see that the regression line is sane but imperfect when compared to the test data. The R-squared is just 0.015 on the test set. That means using the regression model is not much better than just guessing the average from the post-1970 data.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hide code, show output</span>
<span class="n">eq</span> <span class="o">=</span> <span class="sa">r</span><span class="s2">&quot;$\hat</span><span class="si">{y}</span><span class="s2"> = &quot;</span> <span class="o">+</span> <span class="sa">fr</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">+.1f</span><span class="si">}{</span><span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">+.1f</span><span class="si">}</span><span class="s2">x$&quot;</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">vote_loss</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">pres_approval</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">vote_loss</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">pres_approval</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Standardized Vote Loss&quot;</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Presidential Approval Rating&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">eq</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">x0</span><span class="p">,</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">pres_approval</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">df1</span><span class="o">.</span><span class="n">pres_approval</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">x0</span><span class="p">])</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">tufte_approval_only</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">values</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">x1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">],</span> <span class="p">[</span><span class="n">y0</span><span class="p">,</span> <span class="n">y1</span><span class="p">],</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>

<span class="c1"># residuals</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residual by year&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">single_lin_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">single_lin_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Residual&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Year&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Model 1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/4e7e6661aae4bf49a79a9c3c632e00eefeb144658d11cdbfb0f5f48c3e3891e9.png" src="../_images/4e7e6661aae4bf49a79a9c3c632e00eefeb144658d11cdbfb0f5f48c3e3891e9.png" />
</div>
</div>
</section>
<section id="model-2">
<h3>Model 2<a class="headerlink" href="#model-2" title="Link to this heading">#</a></h3>
<p>Model 1 has an R-squared of 0.876 in the training set. From the residual scatter below, we see that the residuals become more volatile over time. Indeed, the R-squared is -0.233 (yes, negative) for the test data. That means using the regression model is worse than just guessing the average from the post-1970 data.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># hide code, show output </span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">vote_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">vote_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Observed&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Predicted vs Actual&quot;</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_predictions</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">multi_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">year</span><span class="p">,</span> <span class="n">df2</span><span class="o">.</span><span class="n">multi_reg_residuals</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Residual&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Year&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Residual by year&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Model 2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/6a7ecba1ee02d9bde09bbf3d83a6ae5c9350a18606259c464e21375ed700194d.png" src="../_images/6a7ecba1ee02d9bde09bbf3d83a6ae5c9350a18606259c464e21375ed700194d.png" />
</div>
</div>
</section>
</section>
<section id="could-tufte-have-produced-a-better-model">
<h2>Could Tufte have produced a better model?<a class="headerlink" href="#could-tufte-have-produced-a-better-model" title="Link to this heading">#</a></h2>
<p>Above we see that model 1 is actually better than model 2 on test data. This is a case where a simple model generalizes to new data better. Should Tufte have known this? Probably not. He didn’t have access to the test data. To perform model selection, he would have needed to reserve some of his eight observations for validation. With more data, we should be able to do better. The next section tries to make that process more clear.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="bias-variance-tradeoff">
<h1>Bias-Variance Tradeoff<a class="headerlink" href="#bias-variance-tradeoff" title="Link to this heading">#</a></h1>
<p>There is in general a tradeoff between a model’s <em>bias</em> and its <em>variance</em>. Bias refers to the difference between the truth and what your model predits. Variance refers to the tendency for the exact model fit to vary according to the randomness of the data you have. Both are bad. We like unbiased models and we want low variance so that our predictions are more exact.</p>
<p>More complicated models tend to have higher variance. As you add more features or polynomial terms to a linear regression, your model is more sensitive to noise. However, a more simplistic model won’t come as close to describing the data generating process as it really is. Thus a simple model is biased.</p>
<p>When we decide on a performance metric like mean squared error (MSE) we are saying that we are willing to trade a reduction in one for an increase in the other. This is somewhat different than an emphasis on unbiased models (OLS is the BLUE) you might be familiar with in other settings.</p>
<section id="biasvariance-decomposition-supervised-prediction">
<h2>Bias–Variance Decomposition (supervised prediction)<a class="headerlink" href="#biasvariance-decomposition-supervised-prediction" title="Link to this heading">#</a></h2>
<p><strong>Data-generating process</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y = f(X) + \varepsilon\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon \mid X] = 0\)</span>, <span class="math notranslate nohighlight">\(\mathbb{Var}(\varepsilon \mid X) = \sigma^2(X)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\hat f\)</span> is a fitted model (random through the training sample/algorithm)</p></li>
</ul>
<p>Define the test MSE at <span class="math notranslate nohighlight">\(x\)</span>:
$<span class="math notranslate nohighlight">\(
\mathrm{MSE}(x) \;=\; \mathbb{E}\!\left[(Y - \hat f(x))^2 \mid X=x\right].
\)</span>$</p>
<p>Substitute <span class="math notranslate nohighlight">\(Y=f(x)+\varepsilon\)</span>:
$<span class="math notranslate nohighlight">\(
\begin{aligned}
\mathrm{MSE}(x)
&amp;= \mathbb{E}\!\left[(f(x)+\varepsilon-\hat f(x))^2 \mid X=x\right] \\
&amp;= \mathbb{E}\!\left[(f(x)-\hat f(x))^2 \mid X=x\right] + \mathbb{E}[\varepsilon^2 \mid X=x] + 2\,\mathbb{E}\!\left[\varepsilon\{f(x)-\hat f(x)\}\mid X=x\right].
\end{aligned}
\)</span>$</p>
<p>Because <span class="math notranslate nohighlight">\(\mathbb{E}[\varepsilon\mid X=x]=0\)</span> and the test noise <span class="math notranslate nohighlight">\(\varepsilon\)</span> is independent of the training randomness in <span class="math notranslate nohighlight">\(\hat f\)</span>, the cross term is <span class="math notranslate nohighlight">\(0\)</span>. Hence
$<span class="math notranslate nohighlight">\(
\mathrm{MSE}(x) \;=\; \underbrace{\mathbb{E}\!\left[(f(x)-\hat f(x))^2\right]}_{\text{estimation error}}
\;+\; \underbrace{\sigma^2(x)}_{\text{irreducible noise}}.
\)</span>$</p>
<p>Now split the estimation error into bias<span class="math notranslate nohighlight">\(^2\)</span> and variance:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{E}\!\left[(f(x)-\hat f(x))^2\right]
&amp;= \mathbb{E}\!\left[(\hat f(x)-\mathbb{E}[\hat f(x)] + \mathbb{E}[\hat f(x)]-f(x))^2\right] \\
&amp;= \mathbb{E}\!\left[(\hat f(x)-\mathbb{E}[\hat f(x)])^2\right] + (\mathbb{E}[\hat f(x)]-f(x))^2 + 2\,\mathbb{E}\!\left[(\hat f(x)-\mathbb{E}[\hat f(x)])(\mathbb{E}[\hat f(x)]-f(x))\right] \\
\end{aligned}
\end{split}\]</div>
<p>The first term is the variance of <span class="math notranslate nohighlight">\(\hat{f}\)</span> by definiton. The second term is the squared bias, by definition. The last, cross term is zero (the second factor is constant with respect to the expectation over training randomness), so</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{E}\!\left[(f(x)-\hat f(x))^2\right] &amp;= \underbrace{\mathbb{Var}(\hat f(x))}_{\text{variance}} + \underbrace{(\mathbb{E}[\hat f(x)]-f(x))^2}_{\text{bias}^2},
\end{aligned}
\]</div>
<p>Therefore,</p>
<div class="math notranslate nohighlight">
\[
\boxed{\ \mathrm{MSE}(x) = \underbrace{(\mathbb{E}[\hat f(x)]-f(x))^2}_{\text{bias}^2(x)} + \underbrace{\mathbb{Var}(\hat f(x))}_{\text{variance}(x)} + \underbrace{\sigma^2(x)}_{\text{irreducible noise}}\ }.
\]</div>
<section id="interpretation">
<h3>Interpretation<a class="headerlink" href="#interpretation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias</strong>: systematic error from model misspecification/rigidity.</p></li>
<li><p><strong>Variance</strong>: sensitivity to the particular training sample.</p></li>
<li><p><strong>Noise</strong>: randomness you cannot remove.</p></li>
</ul>
<p>Making the model more flexible typically reduces bias but increases variance restricting it does the opposite. Optimal predictive performance sits where bias<span class="math notranslate nohighlight">\(^2\)</span> + variance is minimized.</p>
</section>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(f(x) = \sin x + \varepsilon\)</span>, where <span class="math notranslate nohighlight">\(\varepsilon\)</span> is normally distributed, iid noise. Suppose we observe <span class="math notranslate nohighlight">\(x\)</span> drawn uniformly from <span class="math notranslate nohighlight">\([0,\pi]\)</span> so we only have the symmetric inverted-U shape.</p>
<div class="cell tag_hide-input docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell source</p>
<p class="expanded admonition-title">Hide code cell source</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># hide code and show output</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>

<span class="c1">#x = np.linspace(0, np.pi, 1_000)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2_000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">n_axs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n_axs</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;Sine from 0 to $\pi$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\pi/2$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\pi$&quot;</span><span class="p">])</span> 

<span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()</span>

<span class="n">data_sets</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<span class="n">n_sims</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n</span><span class="o">=</span><span class="mi">10</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_sims</span><span class="p">):</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n_axs</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
        
        <span class="c1"># Add both linear and quadratic regression for axs[1] and axs[2]</span>
        <span class="n">p1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">y_lin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_lin</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear fit&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
        
        <span class="n">p2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">y_quad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_quad</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Quadratic fit&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C2&#39;</span><span class="p">)</span>
        
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dataset </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> - Linear &amp; Quadratic Regression&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
        
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/b6d413649c64c2f8491f904e02185f0fb451bee647c27704b6c3e1ebb7d5b5c8.png" src="../_images/b6d413649c64c2f8491f904e02185f0fb451bee647c27704b6c3e1ebb7d5b5c8.png" />
</div>
</div>
<p>In the above, the simpler model will perform much better if the distribution of <span class="math notranslate nohighlight">\(x\)</span> values shifts (technically called covariate shift), so that we start observing values <span class="math notranslate nohighlight">\(x&gt;\pi\)</span>. The more complex, quadratic model is probably better in all other circumstances.</p>
<p>Sometimes the simpler model is better for any sample of <span class="math notranslate nohighlight">\(x\)</span> values from the same distribution. This is a more pure bias variance tradeoff. Suppose instead the actual data generating process is one of pure noise <span class="math notranslate nohighlight">\(g(x) = \varepsilon\)</span>, but that there is a lot of noise so the standard deviation of the noise is high. The quadratic model is <em>correctly specified</em> but, because it is so sensitive to the noise, it might not perform as well as the biased linear model with lower variance. Hence the tradeoff.</p>
<figure class="align-default" id="fig-bias-variance">
<a class="reference internal image-reference" href="../_images/scatter_bias_variance.svg"><img alt="Scatter plot showing bias-variance tradeoff" src="../_images/scatter_bias_variance.svg" style="width: 100%;" />
</a>
</figure>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># hide code and hide output </span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2_000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># True function</span>
<span class="n">n_axs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_cols</span> <span class="o">=</span> <span class="mi">2</span>


<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n_axs</span><span class="p">,</span> <span class="n">n_cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="s1">&#39;col&#39;</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="s1">&#39;row&#39;</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">n_test</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_axs</span><span class="p">):</span>
    <span class="c1"># Generate training data</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">noise_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">noise_train</span>  <span class="c1"># FIXED: y = x^2 + noise</span>
    
    <span class="c1"># Generate test data from same distribution</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_test</span><span class="p">)</span>
    <span class="n">noise_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_test</span><span class="p">)</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">noise_test</span>  <span class="c1"># FIXED: y = x^2 + noise</span>
    
    <span class="c1"># Fit models on training data</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># LEFT COLUMN: Training data</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1">#, label=&#39;True: $y=x^2$&#39;)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    
    <span class="n">y_lin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_lin</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear fit&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
    
    <span class="n">y_quad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_quad</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Quadratic fit&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C2&#39;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training Set </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Removed R² values as requested</span>
    
    <span class="c1"># RIGHT COLUMN: Test data</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True: $y=x^2$&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;lightblue&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    
    <span class="c1"># Apply fitted models to test data</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_lin</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear pred&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_quad</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Quadratic pred&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C2&#39;</span><span class="p">)</span>
    
    <span class="c1"># Calculate test R-squared</span>
    <span class="n">y_lin_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span>
    <span class="n">y_quad_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="n">x_test</span><span class="p">)</span>
    
    <span class="n">ss_tot_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ss_res_lin_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_lin_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ss_res_quad_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_quad_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="n">r2_lin_test</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">ss_res_lin_test</span> <span class="o">/</span> <span class="n">ss_tot_test</span><span class="p">)</span>
    <span class="n">r2_quad_test</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">ss_res_quad_test</span> <span class="o">/</span> <span class="n">ss_tot_test</span><span class="p">)</span>
    
    <span class="c1"># Add comparison to title with R² values</span>
    <span class="k">if</span> <span class="n">r2_quad_test</span> <span class="o">&gt;</span> <span class="n">r2_lin_test</span><span class="p">:</span>
        <span class="n">title_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Test Set </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> - Quad R² = </span><span class="si">{</span><span class="n">r2_quad_test</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> &gt; Lin R² = </span><span class="si">{</span><span class="n">r2_lin_test</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">title_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Test Set </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> - Lin R² = </span><span class="si">{</span><span class="n">r2_lin_test</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> &gt; Quad R² = </span><span class="si">{</span><span class="n">r2_quad_test</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="c1">#ax.legend()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title_text</span><span class="p">)</span>

<span class="c1"># Set labels</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Quadratic Truth with Noise: Training vs Test Performance&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.02</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="c1">#plt.savefig(&quot;../assets/scatter_bias_variance.svg&quot;, transparent=False)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2609238071a4f1eeab79ec9ea21b2d8af68d809e8da3e3be2a3cdedbebbb5e43.png" src="../_images/2609238071a4f1eeab79ec9ea21b2d8af68d809e8da3e3be2a3cdedbebbb5e43.png" />
</div>
</details>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="validation-and-model-selection">
<h1>Validation and Model Selection<a class="headerlink" href="#validation-and-model-selection" title="Link to this heading">#</a></h1>
<p>We have seen how simple, biased models can sometimes generalize better than unbiased models. We have seen that complex models will fit our training data better but fail to perfom well on test data. We have even critiqued Ed Tufte with the benefit of hindsight. Soon, it will be our turn to step into the arena and create our own models. Given that our goal is to produce a model that performs well on new data, we need to do our own validation that this will be the case. The way this is typically done is by <strong>cross validation</strong>. Cross validation has two uses:</p>
<ol class="arabic simple">
<li><p>Obtain a measure of the performance of your model</p></li>
<li><p>Tune your model’s <strong>hyperparameters</strong>. This is called model selection.</p></li>
</ol>
<p>Hyperparameters are model parameters that need to be set by the researcher. Neural networks and other complex models involve many hyperparameters. If you are working with a fixed OLS specification, then there are no hyperparameters. The slope of a regression line is <em>not</em> a hyperparameter, because it is determined by the model training process. However, we might consider the degree of polynomial that we use in a regression to be a hyperparameter.</p>
<section id="simple-holdout-validation">
<h2>Simple Holdout Validation<a class="headerlink" href="#simple-holdout-validation" title="Link to this heading">#</a></h2>
<p>The most basic form of validation goes like this: randomly split the data into training, test, and a <em>validation</em> (aka <em>dev</em>) set. Then</p>
<ol class="arabic simple">
<li><p>Train each model variant (eg different polynomial specifications) on the training set.</p></li>
<li><p>Measure the error on the dev set.</p></li>
<li><p>Pick model with the lowest error on the the dev set.</p></li>
<li><p>Reserve the test set to get a final measure of the model performance.</p></li>
</ol>
<p>One rule of thumb is to use 60% of your data for training, 20% for dev, and 20% for test. With large data, you might keep more data for training. Large is relative to the size of an effect you’re trying to detect, so we won’t make this more precise for now.</p>
<p>This method enables us to perform model selection and to get an estimate of our model’s error. You might find it striking that we <em>make no decisions on the basis of the test set</em>. When data is scarce, this is obviously inconvenient. Why can’t we use the dev set error to report on our model? Each  measurement is a noisy read on the actual model performance. Once we select the model that looks the best on the dev set, we create some bias–we’re selecting for models that perform very well by chance. Of course, we still want to select the model with the best performance, but we shouldn’t be surprised if the model doesn’t do quite as well on the test set. This phenomenon is sometimes referred to as <em>model-selection bias</em>, an instance of the <em>winner’s curse</em>, or simply <em>optimistic bias</em>.</p>
<p>Whenever you select for an extreme, you tend to end up capturing what might be some exceptional noise. As an analogy, consider all of the Forbes 30 Under 30 honorees who have gone to jail. By selecting for success, as it is noisily measured, Forbes ends up honoring a few people who are fraudsters or just very lucky (see Twyman’s Law and regression to the mean).</p>
</section>
<section id="cross-validation-for-model-performance">
<h2>Cross Validation (For Model Performance)<a class="headerlink" href="#cross-validation-for-model-performance" title="Link to this heading">#</a></h2>
<p>With scarce data and enough computing power, cross validation is often preferred, where we rotate the data through training and validation sets. Specifically in <span class="math notranslate nohighlight">\(\mathbf{k}\)</span>-<strong>fold</strong> validation,</p>
<ol class="arabic simple">
<li><p>Randomly plit the data in <span class="math notranslate nohighlight">\(k\)</span> partitions, or folds, of equal size.</p></li>
<li><p>Choose <span class="math notranslate nohighlight">\(k-1\)</span> folds for training.</p></li>
<li><p>Use the remaining fold for validation–for a measure of performance.</p></li>
<li><p>Repeat 2-3 so that each fold is used as a validation set.</p></li>
<li><p>Compute the cross-validation estimate as the size-weighted mean of the fold-level validation errors. If folds are equal size, this reduces to the simple average.
$<span class="math notranslate nohighlight">\(\widehat{\mathrm{CV}}=\sum_{k=1}^K \frac{n_k}{N}\,\mathrm{Err}_k.\)</span>$</p></li>
</ol>
<p>There is a sort of bias variance tradeoff in choosing <span class="math notranslate nohighlight">\(k\)</span>. If <span class="math notranslate nohighlight">\(k\)</span> is small, your final performance estimate is pessimistic because the model is trained on relatively few data points. If <span class="math notranslate nohighlight">\(k\)</span> is large (for instance, in the extreme of <span class="math notranslate nohighlight">\(k=n\)</span>, leave-one-out cross validation), then there is less bias but higher variance. <span class="math notranslate nohighlight">\(k\)</span> between 5 and 10 is usually recommended.</p>
<p>This will generally give you a good measure of your model’s performance on new data <em>as long as you aren’t using this for model selection.</em> <span id="id10">[<a class="reference internal" href="../reference/bibliography.html#id14" title="Sudhir Varma and Richard Simon. Bias in error estimation when using cross-validation for model selection. BMC bioinformatics, 7(1):91, 2006.">Varma and Simon, 2006</a>]</span> shows cross validation is biased in this case, though this isn’t made especially clear in ESL and other texts.</p>
</section>
<section id="nested-cross-validation-for-model-selection-and-model-performance">
<h2>Nested Cross Validation (For Model Selection and Model Performance)<a class="headerlink" href="#nested-cross-validation-for-model-selection-and-model-performance" title="Link to this heading">#</a></h2>
<p>In nested cross validation, we use two layers of cross validation: an inner loop for model selection (hyperparameter tuning) and an outer loop for performance estimation. This approach provides an unbiased estimate of model performance while avoiding the model-selection bias discussed previously.</p>
<p>The process works as follows:</p>
<ol class="arabic simple">
<li><p><strong>Outer Loop</strong> (Performance Estimation):</p>
<ul class="simple">
<li><p>Split data into <span class="math notranslate nohighlight">\(K_{\text{outer}}\)</span> folds</p></li>
<li><p>For each outer fold <span class="math notranslate nohighlight">\(k = 1, ..., K_{outer}\)</span>:</p>
<ul>
<li><p>Hold out fold <span class="math notranslate nohighlight">\(k\)</span> as the test set</p></li>
<li><p>Use the remaining <span class="math notranslate nohighlight">\(K_{outer} - 1\)</span> folds for the inner loop</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Inner Loop</strong> (Model Selection):</p>
<ul class="simple">
<li><p>Split the training data (from outer loop) into <span class="math notranslate nohighlight">\(K_{inner}\)</span> folds</p></li>
<li><p>For each hyperparameter configuration:</p>
<ul>
<li><p>Perform <span class="math notranslate nohighlight">\(K_{inner}\)</span>-fold cross validation</p></li>
<li><p>Calculate average validation performance</p></li>
</ul>
</li>
<li><p>Select the best hyperparameter configuration</p></li>
<li><p>Retrain model with best hyperparameters on all inner loop data</p></li>
</ul>
</li>
<li><p><strong>Evaluation</strong>:</p>
<ul class="simple">
<li><p>Test the model from step 2 on the held-out outer fold</p></li>
<li><p>Repeat for all outer folds</p></li>
<li><p>Report the average performance across all outer folds as the final estimate</p></li>
</ul>
</li>
</ol>
<p>Note, your average performance measure might average over models with different hyperparameter and can only be interpreted as the performance of your general algorithm.</p>
<p>In my experience in industry, a degenerate case of nested cross validation is the most common. <span class="math notranslate nohighlight">\(K_{\text{outer}}\)</span> is usually set to one, so this is just cross-validation over a training+dev set and with a test set that is left untouched until after you have selected your model. This also makes it easier to decide what hyperparameters you should actually pick if you’re deploying a model.</p>
<section id="why-nested-cross-validation">
<h3>Why Nested Cross Validation?<a class="headerlink" href="#why-nested-cross-validation" title="Link to this heading">#</a></h3>
<p>Using regular cross validation for both model selection and performance estimation leads to optimistic bias. When you select the best-performing model based on cross validation scores, you’re essentially “peeking” at the test data through the selection process. The model that performs best might have gotten lucky with the particular data splits.</p>
<p>Nested cross validation separates these concerns:</p>
<ul class="simple">
<li><p>The inner loop finds the best hyperparameters for each outer training set</p></li>
<li><p>The outer loop provides an unbiased estimate of how well this model selection procedure works on truly unseen data</p></li>
</ul>
</section>
</section>
<section id="time-series-unbalanced-data-and-other-complications">
<h2>Time Series, Unbalanced Data, and Other Complications<a class="headerlink" href="#time-series-unbalanced-data-and-other-complications" title="Link to this heading">#</a></h2>
<p>For time series, the use of longitudinal holdouts are essential so that your validation and test sets contains data from a later time period than the data used for training.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h1>
<p>We’ve now learned about the predictive modeling process. Model tuning and selection is prone to the problem of over-fitting. Performance measures like R-squared (and adjusted R-squared) are misleading when calculated based on training data. A model’s performance can only truly be measured by using completely new test data that was not involved in the training process. The researcher needs to remain disciplined and use procedures like cross-validation to make model selection more systematic, transparent, and reliable. <a class="reference external" href="https://www.econtalk.org/susan-athey-on-machine-learning-big-data-and-causation/">As noted by Susan Athey</a>, instead of researchers subjectively choosing variables and testing specifications behind the scenes, we can now explicitly use data to determine which variables matter. This technological shift enables what <span id="id11">[<a class="reference internal" href="../reference/bibliography.html#id10" title="Justin Grimmer, Margaret E Roberts, and Brandon M Stewart. Machine learning for social science: an agnostic approach. Annual Review of Political Science, 24(1):395–419, 2021.">Grimmer <em>et al.</em>, 2021</a>]</span> describe as a move away from purely deductive social science toward a more inductive, iterative approach where researchers can discover patterns in data rather than only testing pre-specified hypotheses.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercises">
<h1>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h1>
<div class="exercise admonition" id="tufte">

<p class="admonition-title"><span class="caption-number">Exercise 3 </span></p>
<section id="exercise-content">
<p>Using <code class="docutils literal notranslate"><span class="pre">tufte_midterms.csv</span></code>, build a few models uising on the data from 1970 and earlier. Compare the test R-squared for the different model specifications. What specification performs best?</p>
</section>
</div>
<div class="exercise admonition" id="CV-selection">

<p class="admonition-title"><span class="caption-number">Exercise 4 </span></p>
<section id="exercise-content">
<p>Lisa and Bart are trying to find the best model to predict the quantities of oil underground. Lisa chooses her model by (1) dividing her data into test and training. (2) She compares the performance of several different models and hyperparameter settings using 5-fold cross validation on the the training set. Each model is trained five times and evaluated on the five different holdout folds. She obtains an estimate of the model performance by averaging over each of the five evaluations. (3) She picks the winning model and tuning parameters based on the CV procedure. (4) She uses the test set at the end to get a final measure of performance for the selected model.</p>
<p>Bart uses a similar procedure but he picks the winner by using the <em>test</em> set. He uses CV to tune different types of models. Then, he compares each tuned model on the test set and picks the model with the best performance.</p>
<p>Who uses cross validation correctly? Lisa? Bart? Both? Neither?</p>
</section>
</div>
<div class="exercise admonition" id="james-stein">

<p class="admonition-title"><span class="caption-number">Exercise 5 </span></p>
<section id="exercise-content">
<p>Consider the estimator: <span class="math notranslate nohighlight">\(\theta_\alpha = (1 - \alpha)Y\)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is a constant in <span class="math notranslate nohighlight">\([0,1]\)</span> and <span class="math notranslate nohighlight">\(Y\in \mathbb{R}^3\)</span> is vector of independent normal random variables where <span class="math notranslate nohighlight">\(Y_i\sim N(\theta_i, 1)\)</span> for <span class="math notranslate nohighlight">\(i=1,2,3\)</span>.</p>
<p>1.) Show that <span class="math notranslate nohighlight">\(\text{MSE}(\alpha) = \mathbb{E}_Y \Vert \theta - \theta_\alpha \Vert^2  = \alpha^2 \Vert \theta \Vert^2 + 3(1-\alpha)^2 \)</span></p>
<p>2.) Find the value of <span class="math notranslate nohighlight">\(\alpha\)</span> that minimizes the MSE.</p>
<p>3.) Show the MSE-optimal <span class="math notranslate nohighlight">\(\theta_\alpha\)</span> is also biased.</p>
</section>
</div>
<div class="exercise admonition" id="phack">

<p class="admonition-title"><span class="caption-number">Exercise 6 </span></p>
<section id="exercise-content">
<p>In what ways are p-hacking and overfitting similar? In what ways are they different?</p>
</section>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "alexanderthclark/pols4728",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="evals_and_prompting.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Using LLMs</p>
      </div>
    </a>
    <a class="right-next"
       href="linreg.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Linear Regression</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Predictive Modeling</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-hat-y">Predicting <span class="math notranslate nohighlight">\(\hat{y}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tufte-s-midterm-elections-model">Tufte’s Midterm Elections Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-1">Model 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-2">Model 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#could-tufte-have-produced-a-better-model">Could Tufte have produced a better model?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-tradeoff">Bias-Variance Tradeoff</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#biasvariance-decomposition-supervised-prediction">Bias–Variance Decomposition (supervised prediction)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation">Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-and-model-selection">Validation and Model Selection</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-holdout-validation">Simple Holdout Validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation-for-model-performance">Cross Validation (For Model Performance)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nested-cross-validation-for-model-selection-and-model-performance">Nested Cross Validation (For Model Selection and Model Performance)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-nested-cross-validation">Why Nested Cross Validation?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-unbalanced-data-and-other-complications">Time Series, Unbalanced Data, and Other Complications</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander Clark, Columbia University
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>