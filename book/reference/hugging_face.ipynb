{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face I\n",
    "\n",
    "This guide shows how to call gpt-oss via Hugging Face Inference Providers (no local GPU). We’ll use the `huggingface_hub` client for chat completion requests.  \n",
    "\n",
    "A model like gpt-oss is too big for my Macbook Pro, so we aren't installing it. Using Hugging Face Inference Providers means requests execute on hosted GPUs; expect shared rate limits and occasional cold starts.\n",
    "\n",
    "If you later want to run models locally, you’ll need `transformers` plus a backend (e.g., PyTorch/MPS) and a much smaller model.\n",
    "\n",
    "## Hugging Face Login\n",
    "\n",
    "First, create a login on [https://huggingface.co/](https://huggingface.co/) for convenience. Create an API token with read permissions. I pasted my token in my `.env` file, adding a line `HF_TOKEN = 'key_here'` so I can use `os.getenv(\"HF_TOKEN\")` instead of pasting the actual key into the program. \n",
    "\n",
    "## Install Python Libraries\n",
    "\n",
    "Install these. The widget libraries might be needed if you are using jupyter. \n",
    "\n",
    "```bash\n",
    "pip install -qU huggingface_hub ipywidgets>=8 jupyterlab_widgets>=3\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "If you see a 429 or 503 error, you're being rate limited or the model is \"cold starting.\" The former can happen if you modify this script to attempt many model requests. This use case will require more sophisticated retry patterns in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from huggingface_hub.errors import HfHubHTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"openai/gpt-oss-20b\"\n",
    "TOKEN = os.getenv(\"HF_TOKEN\")  # or rely on `huggingface-cli login`\n",
    "client = InferenceClient(MODEL, token=TOKEN, timeout=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are a sentiment classifier. Return ONLY 'POSITIVE' or 'NEGATIVE'.\"\n",
    "user_prompt = \"This sandwich is awesome.\"\n",
    "\n",
    "msgs = [ {\"role\":\"system\",\"content\": system_prompt},\n",
    "        {\"role\":\"user\",\"content\": user_prompt} ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "r = client.chat_completion(messages=msgs, max_tokens=64, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You will be given a message from a constituent to a legislator. \"\n",
    "    \"Determine if the message implicitly endorses more or less government spending. \"\n",
    "    \"Return ONLY one value from: \"\n",
    "    \"['increase spending', 'decrease spending'].\" )\n",
    "\n",
    "user_prompt = \"We need more astronauts.\"\n",
    "\n",
    "msgs = [ {\"role\":\"system\",\"content\": system_prompt},\n",
    "        {\"role\":\"user\",\"content\": user_prompt} ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'increase spending'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = client.chat_completion(messages=msgs, max_tokens=128, temperature=0.0)\n",
    "r.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You analyze concerns voiced at a town hall meeting (short texts). \"\n",
    "    \"Identify 1-2 topics (no predefined labels) that best capture the content. \"\n",
    "    \"Return ONLY the topic names as a JSON array of strings.\"\n",
    ")\n",
    "\n",
    "responses = [\n",
    "    \"The bus line was cut and now I can't get to work on time.\",\n",
    "    \"Potholes on Main Street have blown out two of my tires.\",\n",
    "    \"The river is polluted.\",\n",
    "    \"I don't like Facebook and I don't like the Internet.\"\n",
    "]\n",
    "\n",
    "user_prompt = (\n",
    "    \"Task: Discover topics in town-hall concerns and assign each response a mixture over the discovered topics (top-3 only).\\n\"\n",
    "    \"Responses (id: text):\\n\" +\n",
    "    \"\\n\".join(f\"{i+1}: {txt}\" for i, txt in enumerate(responses))\n",
    ")\n",
    "\n",
    "msgs = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "r = client.chat_completion(messages=msgs, max_tokens=2056, temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"1\": [\n",
      "    {\"topic\":\"Public transportation\",\"prob\":0.9},\n",
      "    {\"topic\":\"Road infrastructure\",\"prob\":0.05},\n",
      "    {\"topic\":\"Digital platforms\",\"prob\":0.05}\n",
      "  ],\n",
      "  \"2\": [\n",
      "    {\"topic\":\"Road infrastructure\",\"prob\":0.9},\n",
      "    {\"topic\":\"Public transportation\",\"prob\":0.05},\n",
      "    {\"topic\":\"Environmental pollution\",\"prob\":0.05}\n",
      "  ],\n",
      "  \"3\": [\n",
      "    {\"topic\":\"Environmental pollution\",\"prob\":0.9},\n",
      "    {\"topic\":\"Road infrastructure\",\"prob\":0.05},\n",
      "    {\"topic\":\"Public transportation\",\"prob\":0.05}\n",
      "  ],\n",
      "  \"4\": [\n",
      "    {\"topic\":\"Digital platforms\",\"prob\":0.9},\n",
      "    {\"topic\":\"Public transportation\",\"prob\":0.05},\n",
      "    {\"topic\":\"Road infrastructure\",\"prob\":0.05}\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(r.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Google Gemma\n",
    "\n",
    "Google’s Gemma is a family of low-cost, open-weight large language models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"google/gemma-2-2b-it\"\n",
    "# https://huggingface.co/google/gemma-2-2b-it\n",
    "\n",
    "client_gemma = InferenceClient(MODEL, token=TOKEN, timeout=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the prompt from above\n",
    "r = client_gemma.chat_completion(messages=msgs, max_tokens=2056, temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Transportation\", \"Infrastructure\", \"Environmental\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"infrastructure\", \"transportation\", \"environmental issues\"] 思いさせて頂اگر\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Increase the temperature and get nonsense (apologies for anything obscene)\n",
    "r = client_gemma.chat_completion(messages=msgs, max_tokens=1024, temperature=1.77)\n",
    "print(r.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
