


import os

from huggingface_hub import InferenceClient
from huggingface_hub.errors import HfHubHTTPError


MODEL = "openai/gpt-oss-20b"
TOKEN = os.getenv("HF_TOKEN")  # or rely on `huggingface-cli login`
client = InferenceClient(MODEL, token=TOKEN, timeout=90)





system_prompt = "You are a sentiment classifier. Return ONLY 'POSITIVE' or 'NEGATIVE'."
user_prompt = "This sandwich is awesome."

msgs = [ {"role":"system","content": system_prompt},
        {"role":"user","content": user_prompt} ]


r = client.chat_completion(messages=msgs, max_tokens=64, temperature=0.0)


r.choices[0].message.content





system_prompt = (
    "You will be given a message from a constituent to a legislator. "
    "Determine if the message implicitly endorses more or less government spending. "
    "Return ONLY one value from: "
    "['increase spending', 'decrease spending']." )

user_prompt = "We need more astronauts."

msgs = [ {"role":"system","content": system_prompt},
        {"role":"user","content": user_prompt} ]


r = client.chat_completion(messages=msgs, max_tokens=128, temperature=0.0)
r.choices[0].message.content





system_prompt = (
    "You analyze concerns voiced at a town hall meeting (short texts). "
    "Identify 1-2 topics (no predefined labels) that best capture the content. "
    "Return ONLY the topic names as a JSON array of strings."
)

responses = [
    "The bus line was cut and now I can't get to work on time.",
    "Potholes on Main Street have blown out two of my tires.",
    "The river is polluted.",
    "I don't like Facebook and I don't like the Internet."
]

user_prompt = (
    "Task: Discover topics in town-hall concerns and assign each response a mixture over the discovered topics (top-3 only).\n"
    "Responses (id: text):\n" +
    "\n".join(f"{i+1}: {txt}" for i, txt in enumerate(responses))
)

msgs = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_prompt},
]

r = client.chat_completion(messages=msgs, max_tokens=2056, temperature=0.0)



print(r.choices[0].message.content)





MODEL = "google/gemma-2-2b-it"
# https://huggingface.co/google/gemma-2-2b-it

client_gemma = InferenceClient(MODEL, token=TOKEN, timeout=90)



# Re-use the prompt from above
r = client_gemma.chat_completion(messages=msgs, max_tokens=2056, temperature=0.0)



print(r.choices[0].message.content)


# Increase the temperature and get nonsense
r = client_gemma.chat_completion(messages=msgs, max_tokens=1024, temperature=1.77)
print(r.choices[0].message.content)
