{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ":nosearch:\n\n# Selected Solutions\n\n## [Predictive Modeling](predictive)\n\n```{solution-start} CV-selection\n:class: dropdown\n```\n\nOnly Lisa uses cross-validation correctly. She tunes models and hyperparameters using 5-fold CV on the training data, picks the winner by the cross-validated score, then evaluates that single, refit model once on the untouched test set to obtain an out-of-sample estimate. This approach keeps the test set completely separate from model selection, so the final performance estimate is not biased by the selection process.\n\nBart misuses the test set by selecting the winning model on it after tuning via CV. We already know that we should not tune a specific model using the test set and choosing between models (eg linear regression vs neural network) on the basis of the test set is a similar mistake. This leaks information from the test set into the selection process, which can lead to optimistic bias because the chosen model may perform well on the test set partly due to chance. Once the test set is used for selection, it is no longer a valid measure of true generalization performance. \n\nA better alternative: if you do not want to hold out a single test split,  use nested cross-validation. An inner CV loop is used for tuning and an outer CV loop is used for unbiased performance estimation. This prevents the kind of bias caused by using the test set in the selection process.\n\n```{solution-end}\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}